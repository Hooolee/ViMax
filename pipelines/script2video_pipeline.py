import os
import shutil
import json
import logging
import asyncio
import time
from typing import Optional, Dict, List, Tuple, Literal
from moviepy import VideoFileClip, concatenate_videoclips
from PIL import Image
from agents import *
from agents.best_image_selector import BestImageSelector
import yaml
from interfaces import *
from utils.model_init import init_chat_model_compat
from utils.timer import Timer
import importlib
from utils.timeline import build_timeline, write_timeline_edl, render_timeline

class Script2VideoPipeline:

    # events
    character_portrait_events = {}
    shot_desc_events = {}
    frame_events = {}


    def __init__(
        self,
        chat_model: str,
        image_generator,
        video_generator,
        working_dir: str,
        max_shots: int | None = None,
        interactive_mode: bool = False,
    ):

        self.chat_model = chat_model
        self.image_generator = image_generator
        self.video_generator = video_generator

        self.scene_planner = ScenePlanner(chat_model=self.chat_model)
        self.character_extractor = CharacterExtractor(chat_model=self.chat_model)
        self.character_portraits_generator = CharacterPortraitsGenerator(image_generator=self.image_generator)
        self.storyboard_artist = StoryboardArtist(chat_model=self.chat_model)
        self.camera_image_generator = CameraImageGenerator(chat_model=self.chat_model, image_generator=self.image_generator, video_generator=self.video_generator)
        self.reference_image_selector = ReferenceImageSelector(chat_model=self.chat_model)
        self.best_image_selector = BestImageSelector(chat_model=self.chat_model)

        self.working_dir = working_dir
        self.max_shots = max_shots
        self.interactive_mode = interactive_mode
        os.makedirs(self.working_dir, exist_ok=True)



    @classmethod
    def init_from_config(
        cls,
        config_path: str,
        output_subdir: str | None = None,
    ):
        from utils.config import resolve_env_vars
        with open(config_path, "r") as f:
            config = resolve_env_vars(yaml.safe_load(f))

        chat_model_args = config["chat_model"]["init_args"]
        chat_model = init_chat_model_compat(**chat_model_args)

        image_generator_cls_module, image_generator_cls_name = config["image_generator"]["class_path"].rsplit(".", 1)
        image_generator_cls = getattr(importlib.import_module(image_generator_cls_module), image_generator_cls_name)
        image_generator_args = config["image_generator"]["init_args"]
        image_generator = image_generator_cls(**image_generator_args)

        video_generator_cls_module, video_generator_cls_name = config["video_generator"]["class_path"].rsplit(".", 1)
        video_generator_cls = getattr(importlib.import_module(video_generator_cls_module), video_generator_cls_name)
        video_generator_args = config["video_generator"]["init_args"]
        video_generator = video_generator_cls(**video_generator_args)

        # optional shot limiter for validation/cost control
        max_shots = None
        cfg_max_shots = config.get("max_shots")
        interactive_mode = config.get("interactive_mode", False)
        if isinstance(cfg_max_shots, int) and cfg_max_shots > 0:
            max_shots = cfg_max_shots

        # æ‹¼æ¥å·¥ä½œç›®å½•ï¼šåŸºç¡€è·¯å¾„ + å­ç›®å½•
        base_working_dir = config["working_dir"]
        if output_subdir:
            working_dir = os.path.join(base_working_dir, output_subdir)
        else:
            working_dir = base_working_dir

        return cls(
            chat_model=chat_model,
            image_generator=image_generator,
            video_generator=video_generator,
            working_dir=working_dir,
            max_shots=max_shots,
            interactive_mode=interactive_mode,
        )

    def wait_for_user_confirmation(self, stage_name: str, display_content: str = "") -> str:
        """
        ç­‰å¾…ç”¨æˆ·ç¡®è®¤åç»§ç»­
        
        Args:
            stage_name: å½“å‰é˜¶æ®µåç§°
            display_content: è¦æ˜¾ç¤ºç»™ç”¨æˆ·çš„å†…å®¹
            
        Returns:
            ç”¨æˆ·é€‰æ‹© ('c' ç»§ç»­, 'r' é‡è¯•, 'q' é€€å‡º)
        """
        if not self.interactive_mode:
            return "c"
            
        print("\n" + "="*80)
        print(f"ğŸ“‹ {stage_name}")
        print("="*80)
        if display_content:
            print(display_content)
            print("="*80)
        
        while True:
            choice = input("\nè¯·é€‰æ‹©æ“ä½œ:\n  [c] ç»§ç»­ä¸‹ä¸€æ­¥\n  [r] é‡æ–°ç”Ÿæˆ\n  [q] é€€å‡ºç¨‹åº\n> ").strip().lower()
            if choice in ['c', 'r', 'q']:
                return choice
            print("âŒ æ— æ•ˆè¾“å…¥ï¼Œè¯·è¾“å…¥ cã€r æˆ– q")

    async def __call__(
        self,
        script: str,
        user_requirement: str,
        style: str,
        characters: List[CharacterInScene] = None,
        character_portraits_registry: Optional[Dict[str, Dict[str, Dict[str, str]]]] = None,
        scenes: Optional[List[SceneDefinition]] = None,
    ):
        # ä¿å­˜ style ä¾›åç»­ä½¿ç”¨
        self.style = style
        
        # Step 1: Plan scenes (ç»Ÿä¸€åœºæ™¯åˆ’åˆ†)
        if scenes is not None:
            # ä½¿ç”¨ä¼ å…¥çš„åœºæ™¯å®šä¹‰ï¼ˆæ¥è‡ª Idea2Videoï¼‰
            print(f"ğŸš€ Using {len(scenes)} scene(s) provided by upstream pipeline.")
            # ä¿å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿åç»­ç¼“å­˜
            scenes_path = os.path.join(self.working_dir, "scenes.json")
            if not os.path.exists(scenes_path):
                with open(scenes_path, "w", encoding="utf-8") as f:
                    json.dump([s.model_dump() for s in scenes], f, ensure_ascii=False, indent=4)
                print(f"â˜‘ï¸ Saved {len(scenes)} scene(s) to {scenes_path}.")
        else:
            # æ²¡æœ‰ä¼ å…¥åœºæ™¯å®šä¹‰ï¼ŒæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
            scenes_path = os.path.join(self.working_dir, "scenes.json")
            if os.path.exists(scenes_path):
                with open(scenes_path, "r", encoding="utf-8") as f:
                    from interfaces.scene import SceneDefinition
                    scenes = [SceneDefinition.model_validate(s) for s in json.load(f)]
                print(f"ğŸš€ Loaded {len(scenes)} scene(s) from existing file.")
            else:
                print(f"ğŸ¬ Planning scene segmentation...")
                scenes = await self.plan_scenes(script=script)
                with open(scenes_path, "w", encoding="utf-8") as f:
                    json.dump([s.model_dump() for s in scenes], f, ensure_ascii=False, indent=4)
                print(f"â˜‘ï¸ Planned {len(scenes)} scene(s) and saved to {scenes_path}.")
        
        # ä¿å­˜ scenes ä¾›åç»­ä½¿ç”¨ï¼ˆç”¨äºåœºæ™¯ä¸€è‡´æ€§ï¼‰
        self.scenes = scenes
        self.scenes_dict = {scene.scene_id: scene for scene in scenes}
        
        # Step 2: Extract characters (ä½¿ç”¨ç»Ÿä¸€çš„åœºæ™¯å®šä¹‰)
        if characters is None:
            characters = await self.extract_characters(script=script, scenes=scenes)

            # characters_path = os.path.join(self.working_dir, "characters.json")
            # if os.path.exists(characters_path):
            #     with open(characters_path, "r", encoding="utf-8") as f:
            #         characters = [CharacterInScene.model_validate(c) for c in json.load(f)]
            #     print(f"ğŸš€ Loaded {len(characters)} characters from existing file.")
            # else:
            #     print(f"ğŸ” Extracting characters from script...")
            #     characters = await self.extract_characters(script=script, scenes=scenes)
            #     with open(characters_path, "w", encoding="utf-8") as f:
            #         json.dump([c.model_dump() for c in characters], f, ensure_ascii=False, indent=4)
            #     print(f"â˜‘ï¸ Extracted {len(characters)} characters from script and saved to {characters_path}.")

        # Step 3: Generate character portraits
        if character_portraits_registry is None:
            character_portraits_registry_path = os.path.join(self.working_dir, "character_portraits_registry.json")
            if os.path.exists(character_portraits_registry_path):
                with open(character_portraits_registry_path, "r", encoding="utf-8") as f:
                    character_portraits_registry = json.load(f)
                print(f"ğŸš€ Loaded {len(character_portraits_registry)} character portraits from existing file.")
            else:
                print(f"ğŸ” Generating character portraits...")
                character_portraits_registry = await self.generate_character_portraits(
                    characters=characters,
                    character_portraits_registry=None,
                    style=style,
                )

                with open(character_portraits_registry_path, "w", encoding="utf-8") as f:
                    json.dump(character_portraits_registry, f, ensure_ascii=False, indent=4)
                print(f"â˜‘ï¸ Generated {len(character_portraits_registry)} character portraits and saved to {character_portraits_registry_path}.")



        # Step 4: Design storyboard (ä½¿ç”¨ç»Ÿä¸€çš„åœºæ™¯å®šä¹‰)
        storyboard = await self.design_storyboard(
            script=script,
            characters=characters,
            scenes=scenes,
            user_requirement=user_requirement,
        )

        # decompose visual descriptions of shots
        shot_descriptions = await self.decompose_visual_descriptions(
            shot_brief_descriptions=storyboard,
            characters=characters,
        )

        # construct camera tree
        camera_tree = await self.construct_camera_tree(
            shot_descriptions=shot_descriptions,
        )

        # continuity checks (180/30 degree rules)
        from utils.continuity import check_continuity
        print("ğŸ” Running continuity checks (180/30-degree)...")
        continuity_report = check_continuity(shot_descriptions, camera_tree)
        continuity_report_path = os.path.join(self.working_dir, "continuity_report.json")
        with open(continuity_report_path, "w", encoding="utf-8") as f:
            json.dump(continuity_report, f, ensure_ascii=False, indent=4)
        if not continuity_report.get("passed", False):
            print("âŒ Continuity check failed. See continuity_report.json for details.")
            for v in continuity_report.get("violations", []):
                print(f" - [Shot {v.get('shot_idx')}] {v.get('type')}: {v.get('message')} | å»ºè®®: {v.get('suggestion')}")
            # do not continue to frame generation when failed
            raise RuntimeError("Continuity violations detected; aborting frame generation.")
        else:
            print("âœ… Continuity checks passed.")

        priority_shot_idxs = [camera.parent_cam_idx for camera in camera_tree if camera.parent_cam_idx is not None]
        tasks = [
            self.generate_frames_for_single_camera(
                camera=camera,
                shot_descriptions=shot_descriptions,
                characters=characters,
                character_portraits_registry=character_portraits_registry,
                priority_shot_idxs=priority_shot_idxs,
            )
            for camera in camera_tree
        ]

        # ç­‰å¾…æ‰€æœ‰å¸§ç”Ÿæˆå®Œæˆ
        await asyncio.gather(*tasks)

        # è§†é¢‘ç”Ÿæˆéƒ¨åˆ† - æ ¹æ® interactive_mode å†³å®šæ˜¯å¦é¡ºåºç”Ÿæˆå¹¶äº¤äº’
        if self.interactive_mode:
            # äº¤äº’æ¨¡å¼ï¼šé¡ºåºç”Ÿæˆæ¯ä¸ªåˆ†é•œè§†é¢‘ï¼Œå¹¶ç­‰å¾…ç”¨æˆ·ç¡®è®¤
            for shot_description in shot_descriptions:
                while True:
                    # ç”Ÿæˆå•ä¸ªåˆ†é•œè§†é¢‘
                    await self.generate_video_for_single_shot(
                        shot_description=shot_description,
                    )
                    
                    # å‡†å¤‡æ˜¾ç¤ºå†…å®¹
                    video_path = os.path.join(self.working_dir, "shots", f"{shot_description.idx}", "video.mp4")
                    display_content = f"""
ğŸ¬ åˆ†é•œ #{shot_description.idx} å·²ç”Ÿæˆå®Œæˆ

ğŸ“ åˆ†é•œæè¿°:
  åœºæ™¯: {shot_description.scene_id}
  é•œå¤´å°ºå¯¸: {shot_description.shot_size}
  é•œå¤´è§’åº¦: {shot_description.camera_angle}
  
  é¦–å¸§æè¿°: {shot_description.ff_desc[:100]}...
  è¿åŠ¨æè¿°: {shot_description.motion_desc[:100]}...
  
ğŸ“ è§†é¢‘è·¯å¾„: {video_path}
"""
                    
                    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
                    choice = self.wait_for_user_confirmation(
                        stage_name=f"åˆ†é•œè§†é¢‘ç”Ÿæˆ - ç¬¬ {shot_description.idx + 1}/{len(shot_descriptions)} ä¸ª",
                        display_content=display_content
                    )
                    
                    if choice == 'c':
                        # ç»§ç»­ä¸‹ä¸€ä¸ªåˆ†é•œ
                        print(f"âœ… åˆ†é•œ #{shot_description.idx} å·²ç¡®è®¤ï¼Œç»§ç»­ç”Ÿæˆä¸‹ä¸€ä¸ªåˆ†é•œ...")
                        break
                    elif choice == 'r':
                        # é‡æ–°ç”Ÿæˆ - åˆ é™¤å·²ç”Ÿæˆçš„è§†é¢‘æ–‡ä»¶
                        print(f"ğŸ”„ é‡æ–°ç”Ÿæˆåˆ†é•œ #{shot_description.idx} çš„è§†é¢‘...")
                        if os.path.exists(video_path):
                            os.remove(video_path)
                            print(f"å·²åˆ é™¤æ—§è§†é¢‘: {video_path}")
                        # ç»§ç»­å¾ªç¯é‡æ–°ç”Ÿæˆ
                    elif choice == 'q':
                        print("âš ï¸ ç”¨æˆ·é€‰æ‹©é€€å‡ºï¼Œåœæ­¢è§†é¢‘ç”Ÿæˆæµç¨‹")
                        raise KeyboardInterrupt("ç”¨æˆ·ä¸»åŠ¨é€€å‡º")
        else:
            # éäº¤äº’æ¨¡å¼ï¼šå¹¶å‘ç”Ÿæˆæ‰€æœ‰è§†é¢‘
            video_tasks = [
                self.generate_video_for_single_shot(
                    shot_description=shot_description,
                )
                for shot_description in shot_descriptions
            ]
            await asyncio.gather(*video_tasks)

        final_video_path = os.path.join(self.working_dir, "final_video.mp4")
        timeline_edl_path = os.path.join(self.working_dir, "timeline.edl")
        if os.path.exists(final_video_path) and os.path.exists(timeline_edl_path):
            print(f"ğŸš€ Skipped rendering; final video & EDL already exist.")
        else:
            print(f"ğŸ¬ Building timeline and rendering final video...")
            timeline = build_timeline(shot_descriptions, self.working_dir)
            with open(os.path.join(self.working_dir, "timeline.json"), 'w', encoding='utf-8') as f:
                json.dump(timeline, f, ensure_ascii=False, indent=4)
            write_timeline_edl(timeline, timeline_edl_path)
            render_timeline(timeline, final_video_path)
            print(f"â˜‘ï¸ Rendered final video using timeline, saved to {final_video_path}. EDL: {timeline_edl_path}")

        return final_video_path


    async def generate_frames_for_single_camera(
        self,
        camera: Camera,
        shot_descriptions: List[ShotDescription],
        characters: List[CharacterInScene],
        character_portraits_registry: Dict[str, Dict[str, Dict[str, str]]],
        priority_shot_idxs: List[int],
    ):
        logging.info("="*80)
        logging.info(f"ğŸ–¼ï¸ [Pipeline Stage] Generate Frames for Camera {camera.idx}")
        logging.info("="*80)
        # 1. generate the first_frame of the first shot of the camera
        first_shot_idx = camera.active_shot_idxs[0]
        
        # ç¡®ä¿é•œå¤´ç›®å½•å­˜åœ¨
        first_shot_dir = os.path.join(self.working_dir, "shots", f"{first_shot_idx}")
        os.makedirs(first_shot_dir, exist_ok=True)
        
        first_shot_ff_path = os.path.join(first_shot_dir, "first_frame.png")

        if os.path.exists(first_shot_ff_path):
            print(f"ğŸš€ Skipped generating first_frame for shot {first_shot_idx}, already exists.")
            self.frame_events[first_shot_idx]["first_frame"].set()

        else:
            print(f"ğŸ–¼ï¸ Starting first_frame generation for shot {first_shot_idx}...")
            available_image_path_and_text_pairs = []

            for character_idx in shot_descriptions[first_shot_idx].ff_vis_char_idxs:
                identifier_in_scene = characters[character_idx].identifier_in_scene
                registry_item = character_portraits_registry[identifier_in_scene]
                
                # å¤„ç†æ–°çš„åµŒå¥—ç»“æ„ï¼ˆåŒ…å« appearance_idï¼‰
                # registry_item ç°åœ¨çš„ç»“æ„æ˜¯: {appearance_id: {view: {path, description}}}
                for appearance_or_view, content in registry_item.items():
                    if isinstance(content, dict) and "path" in content:
                        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯ {view: {path, description}}
                        available_image_path_and_text_pairs.append((content["path"], content["description"]))
                    else:
                        # æ–°æ ¼å¼ï¼š{appearance_id: {view: {path, description}}}
                        for view, item in content.items():
                            available_image_path_and_text_pairs.append((item["path"], item["description"]))
            
            # generate the first_frame based on the shot_description.ff_desc
            if camera.parent_shot_idx is not None:
                # generate the first_frame based on the transition video
                parent_shot_idx = camera.parent_shot_idx
                await self.frame_events[parent_shot_idx]["first_frame"].wait()
                parent_shot_ff_path = os.path.join(self.working_dir, "shots", f"{parent_shot_idx}", "first_frame.png")
                transition_video_path = os.path.join(self.working_dir, "shots", f"{first_shot_idx}", f"transition_video_from_shot_{parent_shot_idx}.mp4")

                if os.path.exists(transition_video_path):
                    print(f"ğŸš€ Skipped generating transition video for shot {first_shot_idx} from shot {parent_shot_idx}, already exists.")
                else:
                    print(f"ğŸ–¼ï¸ Starting transition video generation for shot {first_shot_idx} from shot {parent_shot_idx}...")
                    transition_video_output = await self.camera_image_generator.generate_transition_video(
                        first_shot_visual_desc=shot_descriptions[parent_shot_idx].visual_desc,
                        second_shot_visual_desc=shot_descriptions[first_shot_idx].visual_desc,
                        first_shot_ff_path=parent_shot_ff_path,
                    )
                    transition_video_output.save(transition_video_path)
                    print(f"â˜‘ï¸ Generated transition video for shot {first_shot_idx} from shot {parent_shot_idx}, saved to {transition_video_path}.")

                new_camera_image_path = os.path.join(self.working_dir, "shots", f"{first_shot_idx}", f"new_camera_{camera.idx}.png")
                if os.path.exists(new_camera_image_path):
                    print(f"ğŸš€ Skipped generating new camera image for shot {first_shot_idx}, already exists.")
                else:
                    print(f"ğŸ–¼ï¸ Starting new camera image generation for shot {first_shot_idx}...")
                    new_camera_image = self.camera_image_generator.get_new_camera_image(transition_video_path)
                    new_camera_image.save(new_camera_image_path)
                    print(f"â˜‘ï¸ Generated new camera image for shot {first_shot_idx} (not completed), saved to {new_camera_image_path}.")

                    available_image_path_and_text_pairs.append(
                        (
                            new_camera_image_path,
                            f"The composition and background are correct but some elements may be wrong. The wrong elements should be replaced.\nWrong elements: {camera.missing_info}.\nYou must select this image as the main reference and replace the characters in the image with the provided character portraits. Don't change the background."
                        )
                    )


            # å¦‚æœå­é•œå¤´ç¼ºå°‘ä¿¡æ¯ï¼Œåˆ™éœ€è¦é€‰æ‹©å‚è€ƒå›¾åƒç”Ÿæˆ
            if camera.parent_shot_idx is None or camera.missing_info is not None:
                ff_selector_output_path = os.path.join(self.working_dir, "shots", f"{first_shot_idx}", "first_frame_selector_output.json")
                if os.path.exists(ff_selector_output_path):
                    with open(ff_selector_output_path, 'r', encoding='utf-8') as f:
                        ff_selector_output = json.load(f)
                    print(f"ğŸš€ Loaded existing reference image selection and prompt for first_frame of shot {first_shot_idx} from {ff_selector_output_path}.")
                else:
                    print(f"ğŸ” Selecting reference images and generating prompt for first_frame of shot {first_shot_idx}...")
                    ff_selector_output = await self.reference_image_selector.select_reference_images_and_generate_prompt(
                        available_image_path_and_text_pairs=available_image_path_and_text_pairs,
                        frame_description=shot_descriptions[first_shot_idx].ff_desc,
                        style=self.style,  # ä¼ å…¥ style
                        scene_id=shot_descriptions[first_shot_idx].scene_id,  # ä¼ å…¥åœºæ™¯ ID ç”¨äºå¤–è§‚è¿‡æ»¤
                        characters=characters,  # ä¼ å…¥è§’è‰²åˆ—è¡¨ç”¨äºå¤–è§‚è¿‡æ»¤
                    )
                    with open(ff_selector_output_path, 'w', encoding='utf-8') as f:
                        json.dump(ff_selector_output, f, ensure_ascii=False, indent=4)

                    print(f"â˜‘ï¸ Selected reference images and generated prompt for first_frame of shot {first_shot_idx}, saved to {ff_selector_output_path}.")

                reference_image_path_and_text_pairs, prompt = ff_selector_output["reference_image_path_and_text_pairs"], ff_selector_output["text_prompt"]
                prefix_prompt = ""
                for i, (image_path, text) in enumerate(reference_image_path_and_text_pairs):
                    prefix_prompt += f"Image {i}: {text}\n"
                
                # Handle None or empty prompt
                if prompt is None or not prompt.strip():
                    logging.warning(f"text_prompt is None for shot {first_shot_idx} first_frame. Using frame description.")
                    prompt = f"Generate an image based on the following description:\n{shot_descriptions[first_shot_idx].ff_desc}"
                
                prompt = f"{prefix_prompt}\n{prompt}" if prefix_prompt else prompt
                reference_image_paths = [item[0] for item in reference_image_path_and_text_pairs]
                
                # Log the final prompt being sent to image generator
                print(f"\n{'='*80}")
                print(f"ğŸ¨ Generating first_frame for shot {first_shot_idx}")
                print(f"ğŸ“ Final prompt to image generator:")
                print(f"{prompt[:500]}{'...' if len(prompt) > 500 else ''}")
                print(f"ğŸ–¼ï¸  Using {len(reference_image_paths)} reference images")
                print(f"{'='*80}\n")
                
                ff_image: ImageOutput = await self.image_generator.generate_single_image(
                    prompt=prompt,
                    reference_image_paths=reference_image_paths,
                    size="1600x900",
                )
                ff_image.save(first_shot_ff_path)
                self.frame_events[first_shot_idx]["first_frame"].set()
                print(f"â˜‘ï¸ Generated first_frame for shot {first_shot_idx}, saved to {first_shot_ff_path}.")
            else:
                shutil.copy(new_camera_image_path, first_shot_ff_path)
                self.frame_events[first_shot_idx]["first_frame"].set()
                print(f"â˜‘ï¸ Generated first_frame for shot {first_shot_idx}, saved to {first_shot_ff_path}.")


        # 2. generate the following frames of the camera
        # P2 ä¼˜åŒ–ï¼šæ”¹è¿›åŒä¸€ Camera å†…çš„å¸§ç”Ÿæˆé¡ºåºï¼Œç¡®ä¿æ—¶åºä¸€è‡´æ€§
        # ç­–ç•¥ï¼šæŒ‰é•œå¤´é¡ºåºç”Ÿæˆï¼Œæ¯ä¸ªé•œå¤´çš„æœ«å¸§ç”Ÿæˆåå†ç”Ÿæˆä¸‹ä¸€ä¸ªé•œå¤´çš„é¦–å¸§
        priority_tasks = []
        normal_tasks = []
        
        # ç¬¬ä¸€ä¸ªé•œå¤´çš„æœ«å¸§ä¼˜å…ˆç”Ÿæˆï¼ˆå¦‚æœéœ€è¦ï¼‰
        if shot_descriptions[first_shot_idx].variation_type in ["medium", "large"]:
            task = self.generate_frame_for_single_shot(
                shot_idx=first_shot_idx, 
                frame_type="last_frame", 
                first_shot_ff_path_and_text_pair=(first_shot_ff_path, shot_descriptions[first_shot_idx].ff_desc),
                frame_desc=shot_descriptions[first_shot_idx].lf_desc,
                visible_characters=[characters[idx] for idx in shot_descriptions[first_shot_idx].lf_vis_char_idxs],
                character_portraits_registry=character_portraits_registry,
                scene_id=shot_descriptions[first_shot_idx].scene_id,
                shot_descriptions=shot_descriptions,
            )
            # ç«‹å³awaitç¬¬ä¸€ä¸ªé•œå¤´çš„æœ«å¸§ï¼Œç¡®ä¿åç»­é•œå¤´èƒ½çœ‹åˆ°å®ƒ
            await task
            print(f"âœ¨ P2ä¼˜åŒ–: ç¬¬ä¸€ä¸ªé•œå¤´ {first_shot_idx} çš„æœ«å¸§å·²å®Œæˆï¼Œåç»­é•œå¤´ç°åœ¨å¯ä»¥å¼•ç”¨å®ƒ")

        # æŒ‰é•œå¤´é¡ºåºå¤„ç†å…¶ä»–é•œå¤´
        for shot_idx in camera.active_shot_idxs[1:]:
            # ç”Ÿæˆå½“å‰é•œå¤´çš„é¦–å¸§
            first_frame_task = self.generate_frame_for_single_shot(
                    shot_idx=shot_idx, 
                    frame_type="first_frame", 
                    first_shot_ff_path_and_text_pair=(first_shot_ff_path, shot_descriptions[first_shot_idx].ff_desc),
                    frame_desc=shot_descriptions[shot_idx].ff_desc,
                    visible_characters=[characters[idx] for idx in shot_descriptions[shot_idx].ff_vis_char_idxs],
                    character_portraits_registry=character_portraits_registry,
                    scene_id=shot_descriptions[shot_idx].scene_id,
                    shot_descriptions=shot_descriptions,
                )
            
            # å¦‚æœæ˜¯ä¼˜å…ˆçº§é•œå¤´ï¼Œç«‹å³ç­‰å¾…å®Œæˆï¼ˆä¿æŒåŸæœ‰çš„ä¼˜å…ˆçº§é€»è¾‘ï¼‰
            if shot_idx in priority_shot_idxs:
                await first_frame_task
                print(f"âœ¨ P2ä¼˜åŒ–: ä¼˜å…ˆçº§é•œå¤´ {shot_idx} çš„é¦–å¸§å·²å®Œæˆ")
            else:
                normal_tasks.append(first_frame_task)

            # å¦‚æœéœ€è¦æœ«å¸§ï¼Œç”Ÿæˆæœ«å¸§
            if shot_descriptions[shot_idx].variation_type in ["medium", "large"]:
                last_frame_task = self.generate_frame_for_single_shot(
                    shot_idx=shot_idx, 
                    frame_type="last_frame", 
                    first_shot_ff_path_and_text_pair=(first_shot_ff_path, shot_descriptions[first_shot_idx].ff_desc),
                    frame_desc=shot_descriptions[shot_idx].lf_desc,
                    visible_characters=[characters[idx] for idx in shot_descriptions[shot_idx].lf_vis_char_idxs],
                    character_portraits_registry=character_portraits_registry,
                    scene_id=shot_descriptions[shot_idx].scene_id,
                    shot_descriptions=shot_descriptions,
                )
                normal_tasks.append(last_frame_task)

        # ç­‰å¾…æ‰€æœ‰éä¼˜å…ˆçº§ä»»åŠ¡å®Œæˆ
        # æ³¨æ„ï¼šè¿™é‡Œä»ç„¶å¹¶å‘æ‰§è¡Œï¼Œä½†ç”±äº P1 ä¼˜åŒ–ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½èƒ½çœ‹åˆ°å·²å®Œæˆçš„å¸§
        if normal_tasks:
            await asyncio.gather(*normal_tasks)



    async def generate_video_for_single_shot(
        self,
        shot_description: ShotDescription,
    ):
        logging.info("="*80)
        logging.info(f"ğŸ¥ [Pipeline Stage] Generate Video for Shot {shot_description.idx}")
        logging.info("="*80)
        video_path = os.path.join(self.working_dir, "shots", f"{shot_description.idx}", "video.mp4")
        if os.path.exists(video_path):
            print(f"ğŸš€ Skipped generating video for shot {shot_description.idx}, already exists.")
        else:
            # P4 ä¼˜åŒ–ï¼šå¢å¼ºé”™è¯¯å¤„ç†å’Œè¶…æ—¶æœºåˆ¶
            try:
                # ç­‰å¾…é¦–å¸§å®Œæˆï¼Œæ·»åŠ è¶…æ—¶ä¿æŠ¤
                timeout_seconds = 600  # 10åˆ†é’Ÿè¶…æ—¶
                try:
                    await asyncio.wait_for(
                        self.frame_events[shot_description.idx]["first_frame"].wait(),
                        timeout=timeout_seconds
                    )
                    logging.info(f"âœ… P4ä¼˜åŒ–: é•œå¤´ {shot_description.idx} çš„é¦–å¸§å·²å°±ç»ª")
                except asyncio.TimeoutError:
                    logging.error(f"âŒ P4é”™è¯¯: é•œå¤´ {shot_description.idx} é¦–å¸§ç”Ÿæˆè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                    raise RuntimeError(f"Frame generation timeout for shot {shot_description.idx} first_frame")
                
                # å¦‚æœéœ€è¦æœ«å¸§ï¼Œä¹Ÿç­‰å¾…æœ«å¸§å®Œæˆ
                if shot_description.variation_type in ["medium", "large"]:
                    try:
                        await asyncio.wait_for(
                            self.frame_events[shot_description.idx]["last_frame"].wait(),
                            timeout=timeout_seconds
                        )
                        logging.info(f"âœ… P4ä¼˜åŒ–: é•œå¤´ {shot_description.idx} çš„æœ«å¸§å·²å°±ç»ª")
                    except asyncio.TimeoutError:
                        logging.error(f"âŒ P4é”™è¯¯: é•œå¤´ {shot_description.idx} æœ«å¸§ç”Ÿæˆè¶…æ—¶ï¼ˆ{timeout_seconds}ç§’ï¼‰")
                        raise RuntimeError(f"Frame generation timeout for shot {shot_description.idx} last_frame")

                # éªŒè¯å¸§æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨
                frame_paths = []
                first_frame_path = os.path.join(self.working_dir, "shots", f"{shot_description.idx}", "first_frame.png")
                if not os.path.exists(first_frame_path):
                    logging.error(f"âŒ P4é”™è¯¯: é•œå¤´ {shot_description.idx} é¦–å¸§æ–‡ä»¶ä¸å­˜åœ¨: {first_frame_path}")
                    raise FileNotFoundError(f"First frame file not found: {first_frame_path}")
                frame_paths.append(first_frame_path)
                
                if shot_description.variation_type in ["medium", "large"]:
                    last_frame_path = os.path.join(self.working_dir, "shots", f"{shot_description.idx}", "last_frame.png")
                    if not os.path.exists(last_frame_path):
                        logging.error(f"âŒ P4é”™è¯¯: é•œå¤´ {shot_description.idx} æœ«å¸§æ–‡ä»¶ä¸å­˜åœ¨: {last_frame_path}")
                        raise FileNotFoundError(f"Last frame file not found: {last_frame_path}")
                    frame_paths.append(last_frame_path)

                logging.info(f"âœ… P4ä¼˜åŒ–: æ‰€æœ‰å¸§æ–‡ä»¶å·²éªŒè¯å­˜åœ¨ï¼Œå¼€å§‹ç”Ÿæˆè§†é¢‘")
                print(f"ğŸ¬ Starting video generation for shot {shot_description.idx}...")
                video_output = await self.video_generator.generate_single_video(
                    prompt=shot_description.motion_desc + "\n" + shot_description.audio_desc,
                    reference_image_paths=frame_paths,
                )
                video_output.save(video_path)
                print(f"â˜‘ï¸ Generated video for shot {shot_description.idx}, saved to {video_path}.")
                
            except Exception as e:
                logging.error(f"âŒ P4é”™è¯¯: é•œå¤´ {shot_description.idx} è§†é¢‘ç”Ÿæˆå¤±è´¥: {str(e)}")
                # è§¦å‘äº‹ä»¶ä»¥é¿å…æ­»é”ï¼Œä½†æŠ›å‡ºå¼‚å¸¸è®©ä¸Šå±‚å¤„ç†
                raise

    async def generate_frame_for_single_shot(
        self,
        shot_idx: int,
        frame_type: Literal["first_frame", "last_frame"],
        first_shot_ff_path_and_text_pair: Tuple[str, str],
        frame_desc: str,
        visible_characters: List[CharacterInScene],
        character_portraits_registry: Dict[str, Dict[str, Dict[str, str]]],
        scene_id: Optional[int] = None,  # åœºæ™¯ ID
        shot_descriptions: Optional[List[ShotDescription]] = None,  # ç”¨äºæ”¶é›†å·²å®Œæˆçš„å¸§
    ) -> ImageOutput:

        # ç¡®ä¿é•œå¤´ç›®å½•å­˜åœ¨
        shot_dir = os.path.join(self.working_dir, "shots", f"{shot_idx}")
        os.makedirs(shot_dir, exist_ok=True)
        
        frame_image_path = os.path.join(shot_dir, f"{frame_type}.png")

        if os.path.exists(frame_image_path):
            print(f"ğŸš€ Skipped generating {frame_type} for shot {shot_idx}, already exists.")

        else:
            print(f"ğŸ–¼ï¸ Starting {frame_type} generation for shot {shot_idx}...")
            available_image_path_and_text_pairs = []
            
            # è·å–å½“å‰å¸§çš„è§’è‰²æœå‘ä¿¡æ¯
            char_orientations = None
            if frame_type == "first_frame":
                char_orientations = shot_descriptions[shot_idx].ff_char_orientations
            elif frame_type == "last_frame":
                char_orientations = shot_descriptions[shot_idx].lf_char_orientations
            
            # æ ¹æ®è§’è‰²æœå‘é€‰æ‹©å¯¹åº”çš„ä¸‰è§†å›¾
            for visible_character in visible_characters:
                identifier_in_scene = visible_character.identifier_in_scene
                char_idx = visible_character.idx
                registry_item = character_portraits_registry[identifier_in_scene]
                
                # ç¡®å®šåº”è¯¥ä½¿ç”¨å“ªä¸ªè§†è§’
                desired_view = "front"  # é»˜è®¤æ­£é¢
                if char_orientations and char_idx in char_orientations:
                    desired_view = char_orientations[char_idx]
                
                # å¤„ç†æ–°çš„åµŒå¥—ç»“æ„ï¼ˆåŒ…å« appearance_idï¼‰
                for appearance_or_view, content in registry_item.items():
                    if isinstance(content, dict) and "path" in content:
                        # æ—§æ ¼å¼ï¼šç›´æ¥æ˜¯ {view: {path, description}}
                        view = appearance_or_view
                        if view == desired_view:
                            available_image_path_and_text_pairs.append((content["path"], content["description"]))
                            logging.info(f"âœ… ä¼˜åŒ–: ä¸ºè§’è‰² {identifier_in_scene} é€‰æ‹©äº† {desired_view} è§†è§’")
                            break
                    else:
                        # æ–°æ ¼å¼ï¼š{appearance_id: {view: {path, description}}}
                        # åœ¨å½“å‰ appearance ä¸­æŸ¥æ‰¾å¯¹åº”è§†è§’
                        if desired_view in content:
                            item = content[desired_view]
                            available_image_path_and_text_pairs.append((item["path"], item["description"]))
                            logging.info(f"âœ… ä¼˜åŒ–: ä¸ºè§’è‰² {identifier_in_scene} ({appearance_or_view}) é€‰æ‹©äº† {desired_view} è§†è§’")
                            break

            # P1 ä¼˜åŒ–ï¼šæ”¶é›†å·²å®Œæˆçš„å¸§ä½œä¸ºç¯å¢ƒå‚è€ƒï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # åªæ”¶é›†å‰ä¸€ä¸ªé•œå¤´çš„å°¾å¸§ï¼ˆå¦‚æœæ˜¯åŒåœºæ™¯ï¼‰
            current_scene_id = shot_descriptions[shot_idx].scene_id if shot_descriptions and shot_idx < len(shot_descriptions) else None
            
            if shot_idx > 0:
                prev_shot_idx = shot_idx - 1
                prev_scene_id = shot_descriptions[prev_shot_idx].scene_id if shot_descriptions and prev_shot_idx < len(shot_descriptions) else None
                
                # åªæœ‰åŒåœºæ™¯æ‰ä½¿ç”¨å‰ä¸€ä¸ªé•œå¤´çš„å¸§
                if prev_scene_id == current_scene_id:
                    # ä¼˜å…ˆä½¿ç”¨æœ«å¸§ï¼ˆå¦‚æœæœ‰ï¼‰
                    lf_event = self.frame_events.get(prev_shot_idx, {}).get("last_frame")
                    if lf_event and lf_event.is_set():
                        lf_path = os.path.join(self.working_dir, "shots", f"{prev_shot_idx}", "last_frame.png")
                        if os.path.exists(lf_path):
                            available_image_path_and_text_pairs.append((
                                lf_path,
                                f"Previous shot {prev_shot_idx} last frame (for temporal continuity)"
                            ))
                            logging.info(f"âœ… ä¼˜åŒ–: ä½¿ç”¨å‰ä¸€ä¸ªé•œå¤´ #{prev_shot_idx} çš„æœ«å¸§ä½œä¸ºç¯å¢ƒå‚è€ƒ")
                    else:
                        # æ²¡æœ‰æœ«å¸§ï¼Œä½¿ç”¨é¦–å¸§
                        ff_event = self.frame_events.get(prev_shot_idx, {}).get("first_frame")
                        if ff_event and ff_event.is_set():
                            ff_path = os.path.join(self.working_dir, "shots", f"{prev_shot_idx}", "first_frame.png")
                            if os.path.exists(ff_path):
                                available_image_path_and_text_pairs.append((
                                    ff_path,
                                    f"Previous shot {prev_shot_idx} first frame (for temporal continuity)"
                                ))
                                logging.info(f"âœ… ä¼˜åŒ–: ä½¿ç”¨å‰ä¸€ä¸ªé•œå¤´ #{prev_shot_idx} çš„é¦–å¸§ä½œä¸ºç¯å¢ƒå‚è€ƒ")
                else:
                    logging.info(f"âš ï¸ åœºæ™¯åˆ‡æ¢: é•œå¤´ #{shot_idx} (åœºæ™¯{current_scene_id}) ä¸å‰ä¸€é•œå¤´ #{prev_shot_idx} (åœºæ™¯{prev_scene_id}) ä¸åœ¨åŒä¸€åœºæ™¯ï¼Œä¸ä½¿ç”¨å‰ä¸€é•œå¤´çš„å¸§")
            
            # P4 ä¼˜åŒ–ï¼šCamera ç©ºé—´ä¸€è‡´æ€§é”šç‚¹
            # å¯¹äºé•¿é•œå¤´åºåˆ—ï¼ˆCamera å†…é•œå¤´æ•° >= 5ï¼‰ï¼Œä¸”å½“å‰é•œå¤´ä¸é¦–é•œå¤´é—´éš” >= 3ï¼Œ
            # åœ¨åŒåœºæ™¯çš„æƒ…å†µä¸‹ï¼Œæ·»åŠ  Camera é¦–é•œå¤´çš„é¦–å¸§ä½œä¸ºç©ºé—´å‚è€ƒ
            if first_shot_ff_path_and_text_pair is not None:
                first_shot_ff_path, first_shot_ff_desc = first_shot_ff_path_and_text_pair
                
                # ä»è·¯å¾„ä¸­æå–é¦–é•œå¤´çš„ shot_idx
                # è·¯å¾„æ ¼å¼ï¼š/path/to/working_dir/shots/{shot_idx}/first_frame.png
                import re
                match = re.search(r'/shots/(\d+)/first_frame\.png', first_shot_ff_path)
                if match:
                    first_shot_idx = int(match.group(1))
                    first_shot_scene_id = shot_descriptions[first_shot_idx].scene_id if shot_descriptions and first_shot_idx < len(shot_descriptions) else None
                    
                    # è®¡ç®—å½“å‰é•œå¤´ä¸é¦–é•œå¤´çš„é—´éš”
                    shot_gap = shot_idx - first_shot_idx
                    
                    # æ¡ä»¶ï¼š
                    # 1. ä¸æ˜¯é¦–é•œå¤´æœ¬èº«
                    # 2. é—´éš” >= 3ï¼ˆé¿å…ä¸ P1 ä¼˜åŒ–é‡å¤ï¼‰
                    # 3. åŒä¸€åœºæ™¯
                    # 4. é¦–å¸§å·²ç”Ÿæˆ
                    if (shot_idx != first_shot_idx 
                        and shot_gap >= 3 
                        and current_scene_id == first_shot_scene_id
                        and os.path.exists(first_shot_ff_path)):
                        
                        available_image_path_and_text_pairs.append((
                            first_shot_ff_path,
                            f"Camera spatial anchor: first shot {first_shot_idx} first frame (for spatial consistency)"
                        ))
                        logging.info(f"âœ… P4ä¼˜åŒ–: é•œå¤´ #{shot_idx} è·ç¦»é¦–é•œå¤´ #{first_shot_idx} é—´éš”{shot_gap}ï¼Œæ·»åŠ Cameraç©ºé—´é”šç‚¹")
                    else:
                        if shot_gap < 3:
                            logging.debug(f"P4è·³è¿‡: é•œå¤´ #{shot_idx} è·ç¦»é¦–é•œå¤´ä»…{shot_gap}ä¸ªé•œå¤´ï¼Œç”±P1ä¼˜åŒ–è¦†ç›–")
                        elif current_scene_id != first_shot_scene_id:
                            logging.debug(f"P4è·³è¿‡: é•œå¤´ #{shot_idx} (åœºæ™¯{current_scene_id}) ä¸é¦–é•œå¤´ (åœºæ™¯{first_shot_scene_id}) ä¸åœ¨åŒä¸€åœºæ™¯")
            
            # P3 ä¼˜åŒ–ï¼šå¢å¼ºåœºæ™¯å®šä¹‰ä¼ é€’çš„é˜²å¾¡æ€§æ£€æŸ¥
            scene_definition = None
            if scene_id is not None:
                if not hasattr(self, 'scenes_dict'):
                    logging.warning(f"âš ï¸ P3è­¦å‘Š: scenes_dict æœªåˆå§‹åŒ–ï¼Œé•œå¤´ {shot_idx} å°†ç¼ºå°‘åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯")
                else:
                    scene_definition = self.scenes_dict.get(scene_id)
                    if scene_definition is None:
                        logging.warning(f"âš ï¸ P3è­¦å‘Š: åœºæ™¯ID {scene_id} åœ¨ scenes_dict ä¸­ä¸å­˜åœ¨ï¼Œé•œå¤´ {shot_idx} å°†ç¼ºå°‘åœºæ™¯ä¸Šä¸‹æ–‡ä¿¡æ¯")
                    else:
                        logging.info(f"âœ… P3ä¼˜åŒ–: æˆåŠŸè·å–åœºæ™¯ {scene_id} çš„å®šä¹‰ç”¨äºé•œå¤´ {shot_idx}")
            else:
                logging.warning(f"âš ï¸ P3è­¦å‘Š: é•œå¤´ {shot_idx} æ²¡æœ‰å…³è”çš„åœºæ™¯ID")

            selector_output_path = os.path.join(self.working_dir, "shots", f"{shot_idx}", f"{frame_type}_selector_output.json")
            if os.path.exists(selector_output_path):
                with open(selector_output_path, 'r', encoding='utf-8') as f:
                    selector_output = json.load(f)
                print(f"ğŸš€ Loaded existing reference image selection and prompt for {frame_type} frame of shot {shot_idx} from {selector_output_path}.")
            else:
                print(f"ğŸ” Selecting reference images and generating prompt for {frame_type} frame of shot {shot_idx}...")
                selector_output = await self.reference_image_selector.select_reference_images_and_generate_prompt(
                    available_image_path_and_text_pairs=available_image_path_and_text_pairs,
                    frame_description=frame_desc,
                    style=self.style,  # ä¼ å…¥ style
                    scene_id=scene_id,  # ä¼ å…¥åœºæ™¯ ID ç”¨äºå¤–è§‚è¿‡æ»¤
                    characters=visible_characters,  # ä¼ å…¥å¯è§è§’è‰²åˆ—è¡¨
                    scene_definition=scene_definition,  # ä¼ å…¥åœºæ™¯å®šä¹‰ç”¨äºåœºæ™¯ä¸€è‡´æ€§
                )
                with open(selector_output_path, 'w', encoding='utf-8') as f:
                    json.dump(selector_output, f, ensure_ascii=False, indent=4)
                print(f"â˜‘ï¸ Selected reference images and generated prompt for {frame_type} frame of shot {shot_idx}, saved to {selector_output_path}.")

            reference_image_path_and_text_pairs, prompt = selector_output["reference_image_path_and_text_pairs"], selector_output["text_prompt"]
            prefix_prompt = ""
            for i, (image_path, text) in enumerate(reference_image_path_and_text_pairs):
                prefix_prompt += f"Image {i}: {text}\n"
            
            # Handle None or empty prompt
            if prompt is None or not prompt.strip():
                logging.warning(f"text_prompt is None for shot {shot_idx} {frame_type}. Using frame description.")
                prompt = f"Generate an image based on the following description:\n{frame_desc}"
            
            prompt = f"{prefix_prompt}\n{prompt}" if prefix_prompt else prompt
            reference_image_paths = [item[0] for item in reference_image_path_and_text_pairs]

            # Log the final prompt being sent to image generator
            print(f"\n{'='*80}")
            print(f"ğŸ¨ Generating {frame_type} for shot {shot_idx}")
            print(f"ğŸ“ Final prompt to image generator:")
            print(f"{prompt[:500]}{'...' if len(prompt) > 500 else ''}")
            print(f"ğŸ–¼ï¸  Using {len(reference_image_paths)} reference images")
            print(f"{'='*80}\n")

            # multi-sample and select best
            n_candidates = 3
            shot_dir = os.path.join(self.working_dir, "shots", f"{shot_idx}")
            candidate_paths = []
            for k in range(n_candidates):
                candidate_output: ImageOutput = await self.image_generator.generate_single_image(
                    prompt=prompt,
                    reference_image_paths=reference_image_paths,
                    size="1600x900",
                )
                candidate_path = os.path.join(shot_dir, f"{frame_type}_candidate_{k}.png")
                candidate_output.save(candidate_path)
                candidate_paths.append(candidate_path)

            # select best using BestImageSelector
            reference_image_path_and_text_pairs = selector_output["reference_image_path_and_text_pairs"]
            selection_reason = {
                "selected": None,
                "reason": None,
                "candidates": candidate_paths,
            }
            try:
                best_path = await self.best_image_selector(
                    reference_image_path_and_text_pairs=reference_image_path_and_text_pairs,
                    target_description=frame_desc,
                    candidate_image_paths=candidate_paths,
                )
                selection_reason["selected"] = best_path
                selection_reason["reason"] = getattr(self.best_image_selector, "last_reason", None)
            except Exception as e:
                print(f"âš ï¸ Best image selection failed for shot {shot_idx} {frame_type}, fallback to first candidate. Error: {e}")
                best_path = candidate_paths[0]
                selection_reason["selected"] = best_path
                selection_reason["reason"] = f"fallback_first_candidate_due_to_error: {e}"

            # persist reason and finalize chosen frame
            selection_reason_path = os.path.join(shot_dir, f"{frame_type}_selection_reason.json")
            with open(selection_reason_path, 'w', encoding='utf-8') as f:
                json.dump(selection_reason, f, ensure_ascii=False, indent=4)
            shutil.copy(best_path, frame_image_path)
            print(f"â˜‘ï¸ Generated {frame_type} frame for shot {shot_idx}, saved to {frame_image_path} (best of {n_candidates}).")


        self.frame_events[shot_idx][frame_type].set()
        return frame_image_path


    async def construct_camera_tree(
        self,
        shot_descriptions: List[ShotDescription],
    ):
        logging.info("="*80)
        logging.info("ğŸ¥ [Pipeline Stage] Construct Camera Tree")
        logging.info("="*80)
        camera_tree_path = os.path.join(self.working_dir, "camera_tree.json")

        if os.path.exists(camera_tree_path):
            with open(camera_tree_path, "r", encoding="utf-8") as f:
                camera_tree = json.load(f)
            camera_tree = [Camera.model_validate(camera) for camera in camera_tree]
            print(f"ğŸš€ Loaded {len(camera_tree)} cameras from existing file.")
            return camera_tree

        cameras: List[Camera] = []
        for shot_description in shot_descriptions:
            if shot_description.cam_idx not in [camera.idx for camera in cameras]:
                cameras.append(Camera(idx=shot_description.cam_idx, active_shot_idxs=[shot_description.idx]))
            else:
                cameras[shot_description.cam_idx].active_shot_idxs.append(shot_description.idx)

        camera_tree = await self.camera_image_generator.construct_camera_tree(cameras=cameras, shot_descs=shot_descriptions)
        with open(camera_tree_path, "w", encoding="utf-8") as f:
            json.dump([camera.model_dump() for camera in camera_tree], f, ensure_ascii=False, indent=4)
        print(f"âœ… Constructed camera tree and saved to {camera_tree_path}.")
        return camera_tree




    async def plan_scenes(
        self,
        script: str,
    ):
        """
        è§„åˆ’åœºæ™¯åˆ’åˆ†
        
        Plan scene segmentation from the script.
        """
        from interfaces.scene import SceneDefinition
        
        logging.info("="*80)
        logging.info(f"ğŸ¬ [Pipeline Stage] Planning Scene Segmentation")
        logging.info("="*80)
        
        scenes = await self.scene_planner.plan_scenes(script)
        
        return scenes


    async def extract_characters(
        self,
        script: str,
        scenes: List = None,
    ):
        """
        æå–äººç‰©ä¿¡æ¯
        
        Extract character information from the script.
        If scenes are provided, characters will use these scene IDs.
        """
        from interfaces.scene import SceneDefinition
        
        save_path = os.path.join(self.working_dir, "characters.json")

        if os.path.exists(save_path):
            with open(save_path, "r", encoding="utf-8") as f:
                characters = json.load(f)
            characters = [CharacterInScene.model_validate(character) for character in characters]
            print(f"ğŸš€ Loaded {len(characters)} characters from existing file.")
        else:
            characters = await self.character_extractor.extract_characters(script, scenes=scenes)
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump([character.model_dump() for character in characters], f, ensure_ascii=False, indent=4)
            print(f"âœ… Extracted {len(characters)} characters from script and saved to {save_path}.")

        for character in characters:
            self.character_portrait_events[character.idx] = asyncio.Event()

        return characters


    async def generate_character_portraits(
        self,
        characters: List[CharacterInScene],
        character_portraits_registry: Optional[Dict[str, Dict[str, Dict[str, str]]]],
        style: str,
    ):
        character_portraits_registry_path = os.path.join(self.working_dir, "character_portraits_registry.json")
        if character_portraits_registry is None:
            if os.path.exists(character_portraits_registry_path):
                with open(character_portraits_registry_path, 'r', encoding='utf-8') as f:
                    character_portraits_registry = json.load(f)
            else:
                character_portraits_registry = {}


        tasks = [
            self.generate_portraits_for_single_character(character, style)
            for character in characters
            if character.identifier_in_scene not in character_portraits_registry
        ]
        if tasks:
            for future in asyncio.as_completed(tasks):
                character_portraits_registry.update(await future)
                with open(character_portraits_registry_path, 'w', encoding='utf-8') as f:
                    json.dump(character_portraits_registry, f, ensure_ascii=False, indent=4)

            print(f"âœ… Completed character portrait generation for {len(characters)} characters.")
        else:
            print("ğŸš€ All characters already have portraits, skipping portrait generation.")
        return character_portraits_registry


    async def generate_portraits_for_single_character(
        self,
        character: CharacterInScene,
        style: str,
    ):
        """ä¸ºå•ä¸ªè§’è‰²ç”Ÿæˆæ‰€æœ‰å¤–è§‚çš„è‚–åƒ
        
        è¯¥æ–¹æ³•ä¼šä¸ºè§’è‰²çš„æ¯ä¸ªå¤–è§‚ç”Ÿæˆç‹¬ç«‹çš„ä¸‰è§†å›¾è‚–åƒï¼Œå¹¶ä¿å­˜åˆ°å¯¹åº”çš„ç›®å½•ä¸­ã€‚
        """
        character_base_dir = os.path.join(
            self.working_dir, 
            "character_portraits", 
            f"{character.idx}_{character.identifier_in_scene}"
        )
        os.makedirs(character_base_dir, exist_ok=True)

        result = {character.identifier_in_scene: {}}

        # ä¸ºæ¯ä¸ªå¤–è§‚ç”Ÿæˆè‚–åƒ
        for appearance in character.appearances:
            appearance_dir = os.path.join(character_base_dir, appearance.appearance_id)
            os.makedirs(appearance_dir, exist_ok=True)

            front_portrait_path = os.path.join(appearance_dir, "front.png")
            side_portrait_path = os.path.join(appearance_dir, "side.png")
            back_portrait_path = os.path.join(appearance_dir, "back.png")

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if all(os.path.exists(p) for p in [front_portrait_path, side_portrait_path, back_portrait_path]):
                print(f"ğŸš€ Skipped generating portraits for {character.identifier_in_scene} - {appearance.appearance_id}, already exists.")
            else:
                print(f"ğŸ¨ Generating portraits for {character.identifier_in_scene} - {appearance.appearance_id}...")
                
                # ç”Ÿæˆæ­£é¢è‚–åƒ
                if not os.path.exists(front_portrait_path):
                    front_portrait_output = await self.character_portraits_generator.generate_front_portrait(
                        character, style, appearance
                    )
                    front_portrait_output.save(front_portrait_path)

                # ç”Ÿæˆä¾§é¢è‚–åƒ
                if not os.path.exists(side_portrait_path):
                    side_portrait_output = await self.character_portraits_generator.generate_side_portrait(
                        character, front_portrait_path, appearance
                    )
                    side_portrait_output.save(side_portrait_path)

                # ç”ŸæˆèƒŒé¢è‚–åƒ
                if not os.path.exists(back_portrait_path):
                    back_portrait_output = await self.character_portraits_generator.generate_back_portrait(
                        character, front_portrait_path
                    )
                    back_portrait_output.save(back_portrait_path)

                print(f"â˜‘ï¸ Completed portraits for {character.identifier_in_scene} - {appearance.appearance_id}")

            # æ·»åŠ åˆ°ç»“æœä¸­
            # æ³¨æ„ï¼šè¿™é‡Œçš„ key æ ¼å¼å˜æ›´ä¸ºåŒ…å« appearance_id
            appearance_key = appearance.appearance_id
            if appearance_key not in result[character.identifier_in_scene]:
                result[character.identifier_in_scene][appearance_key] = {}
            
            scenes_str = f"scenes {appearance.scene_ids}" if appearance.scene_ids else "all scenes"
            result[character.identifier_in_scene][appearance_key] = {
                "front": {
                    "path": front_portrait_path,
                    "description": f"A front view portrait of {character.identifier_in_scene} ({appearance.description}, {scenes_str}).",
                },
                "side": {
                    "path": side_portrait_path,
                    "description": f"A side view portrait of {character.identifier_in_scene} ({appearance.description}, {scenes_str}).",
                },
                "back": {
                    "path": back_portrait_path,
                    "description": f"A back view portrait of {character.identifier_in_scene} ({appearance.description}, {scenes_str}).",
                },
            }

        self.character_portrait_events[character.idx].set()
        print(f"âœ… Completed all appearance portraits for {character.identifier_in_scene} ({len(character.appearances)} appearance(s)).")

        return result




    async def design_storyboard(
        self,
        script: str,
        characters: List[CharacterInScene],
        scenes: List = None,
        user_requirement: str = None,
    ):
        """
        è®¾è®¡åˆ†é•œ
        
        Design storyboard based on the script.
        If scenes are provided, shots will be assigned to these scene IDs.
        """
        from interfaces.scene import SceneDefinition
        
        logging.info("="*80)
        logging.info("ğŸ“‹ [Pipeline Stage] Design Storyboard")
        logging.info("="*80)
        storyboard_path = os.path.join(self.working_dir, "storyboard.json")
        if os.path.exists(storyboard_path):
            with open(storyboard_path, 'r', encoding='utf-8') as f:
                storyboard = json.load(f)
            storyboard = [ShotBriefDescription.model_validate(shot) for shot in storyboard]
            print(f"ğŸš€ Loaded {len(storyboard)} shot brief descriptions from existing file.")
        else:
            print(f"ğŸ” Designing storyboard...")
            storyboard = await self.storyboard_artist.design_storyboard(
                script=script,
                characters=characters,
                scenes=scenes,
                user_requirement=user_requirement,
                retry_timeout=150,
            )
            with open(storyboard_path, 'w', encoding='utf-8') as f:
                json.dump([shot.model_dump() for shot in storyboard], f, ensure_ascii=False, indent=4)
            print(f"âœ… Designed storyboard and saved to {storyboard_path}.")


        # apply shot limit if configured
        if self.max_shots is not None:
            storyboard = storyboard[: self.max_shots]

        for shot_brief_description in storyboard:
            self.shot_desc_events[shot_brief_description.idx] = asyncio.Event()

        return storyboard



    async def decompose_visual_descriptions(
        self,
        shot_brief_descriptions: List[ShotBriefDescription],
        characters: List[CharacterInScene],
    ):
        logging.info("="*80)
        logging.info("ğŸ¬ [Pipeline Stage] Decompose Visual Descriptions")
        logging.info("="*80)
        tasks = [
            self.decompose_visual_description_for_single_shot_brief_description(shot_brief_description, characters)
            for shot_brief_description in shot_brief_descriptions
        ]

        shot_descriptions = await asyncio.gather(*tasks)
        return shot_descriptions


    async def decompose_visual_description_for_single_shot_brief_description(
        self,
        shot_brief_description: ShotBriefDescription,
        characters: List[CharacterInScene],
    ):
        shot_description_path = os.path.join(self.working_dir, "shots", f"{shot_brief_description.idx}", "shot_description.json")
        os.makedirs(os.path.dirname(shot_description_path), exist_ok=True)

        if os.path.exists(shot_description_path):
            with open(shot_description_path, 'r', encoding='utf-8') as f:
                shot_description = ShotDescription.model_validate(json.load(f))
            print(f"ğŸš€ Loaded shot {shot_brief_description.idx} description from existing file.")
        else:
            shot_description = await self.storyboard_artist.decompose_visual_description(
                shot_brief_desc=shot_brief_description,
                characters=characters,
                retry_timeout=120,
            )
            with open(shot_description_path, 'w', encoding='utf-8') as f:
                json.dump(shot_description.model_dump(), f, ensure_ascii=False, indent=4)
            print(f"âœ… Decomposed visual description for shot {shot_brief_description.idx} and saved to {shot_description_path}.")

        self.shot_desc_events[shot_brief_description.idx].set()

        if shot_description.variation_type in ["medium", "large"]:
            self.frame_events[shot_brief_description.idx] = {
                "first_frame": asyncio.Event(),
                "last_frame": asyncio.Event(),
            }
        else:
            self.frame_events[shot_brief_description.idx] = {
                "first_frame": asyncio.Event(),
            }

        return shot_description
